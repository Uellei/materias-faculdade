# RESUMO PARA PROVA N2
## Método da Divisão e Conquista
### Motivação
É Preciso resolver um problema com uma entrada grande, para facilitar a resolução, a entrada é quebrada em pedaçoes menosres(`DIVISÃO`). Cada pedaço da entrada é então tratado separadamente(`CONQUISTA`), ao final, os resultados parciais são combinados para gerar o resultado final procurado.
### Paradigmaa
Algoritmos baseados em divisão e conquista são, em geral, `recursivos`. A maioria dos algoritmos de divisão e conquista divide o problema em subproblemas da mesma natureza, de tamanho semelhante.
- Um ponto muito importante no projeto de um algoritmo do tipo dividir e conquistar é o balanceamento dos subproblemas, pois ao gerar problemas de tamanhos muito diferentes afeta a complexidade de resolução. O ideal é que os subproblemas gerados tenham tamanhos iguais ou aproximadamente iguais.
### Quando utilizar?
Existem três condições que indicam que a estratégia de divisão e conquista pode ser utilizada com sucesso:
1. Deve ser possível decompor um problema em subproblemas independentes;
2. A combinação dos resultados deve ser eficiente;
3. Os subproblemas devem ser mais ou menos do mesmo tamanho.
### Vantagens
- Resolução de problemas difíceis
- Pode gerar algoritmos eficientes
    - Forte tendência a complexidade logarítmica
- Paralelismo
    - Facilmente paralelizável na fase de conquista
### Desvantagens
- Recursão
- Tamanho da Pilha
    - Números de chamadas recursivas e/ou armazenadas na pilha pode ser um inconveniente.
- Subproblemas Repetidos
    - Nenhum tipo de informação sobre a resolução de subproblemas é utilizada durante o processo. Desta forma, subproblemas repetidos são resolvidos repetidamente.
## Merge Sort
Algoritmo de ordenação baseado no método da divsão e conquista. Enquanto o Quick Sort utiliza o conceito de elemento pivô para dividir o problema em subproblemas, o algoritmo Merge Sort sempre divide o problema de forma balanceada.
### Funcionamento
1. Dividir o vetor em duas partes;
2. Ordenar as duas partes usando chamadas recursivas;
3. Intervalar as duas partes, obtendo um conjunto ordenado de todos os elementos.

<img src="https://miro.medium.com/v2/0*K7cD17vfL7FdTTLK.gif">

- A altura h da árvore de execução é `O(log n)`;
- A quantidade de operações em cada nível é assintoticamente igual a `O(n)`;
- Logo o altorimo é `O(n log n)`.

### Merge Sort - Implementação Recursiva
```py
def merge_sort(v):
    merge_sort_ordena(v, 0, len(v) - 1)

def merge_sort_ordena(v, esq, dir):
    if esq == dir:
        return

    meio = (esq + dir) // 2
    merge_sort_ordena(v, esq, meio)
    merge_sort_ordena(v, meio + 1, dir)
    merge_sort_intercala(v, esq, meio, dir)

def merge_sort_intercala(v, esq, meio, dir):
    i, j, k = 0, 0, esq
    a_tam = meio - esq + 1
    b_tam = dir - meio

    # Create temporary arrays
    a = [v[esq + i] for i in range(a_tam)]
    b = [v[meio + 1 + j] for j in range(b_tam)]

    i, j = 0, 0

    while i < a_tam and j < b_tam:
        if a[i].chave <= b[j].chave:
            v[k] = a[i]
            i += 1
        else:
            v[k] = b[j]
            j += 1
        k += 1

    while i < a_tam:
        v[k] = a[i]
        i += 1
        k += 1

    while j < b_tam:
        v[k] = b[j]
        j += 1
        k += 1
```

### Vantagens
- Complexidade `O(n log n)`;
- Indicado para aplicações que tem restrição de tempo(executa sempre em um determinado tempo para n).
### Desvantagens
- Utiliza memória auxiliar, em `O(n)`;
- Na prática é mais lento que o `Quick Sort` no caso médio.

### Merge Sort - Iterativo
```py
# Ordena o vetor v[0..n-1]
# A cada iteração, intercala dois "blocos" de b elementos:
#  o primeiro com o segundo, o terceiro com o quarto, etc.
def merge_sort_iter(v):
    n = len(v)
    b = 1

    while b < n:
        esq = 0

        while esq + b < n:
            dir = esq + 2 * b
            if dir > n:
                dir = n
            intercala(v, esq, esq + b - 1, dir - 1)
            esq = esq + 2 * b

        b *= 2
```
> Apesar de mais simples de entender e implementar, a versão recursiva requer memória adicional, se comparada a versão iterativa.

## Quick Sort
- Proposto por Hoare em 1960 e publicado em 1962, é o algoritmo de ordenação interna mais rápido que se conhece para uma apla variedade de situações, provavelmente é o mais utilizado. A ideia básica é dividir o problema de ordenar um conjunto com n itens em dois problemas menores. Os problemas menores são ordenados independentemente, os resultados são combinador para produzir a solução final.
- A parte mais delicada do método é o processo de partição. O vetor v[esq..dir] é rearranjado por meio da escolha arbitrária de um pivô x. O vetor A é particionado em duas partes:
    - A parte esquerda com chaves menores ou iguais a x.
    - A parte direita com chaves maiores ou iguais a x.
#### Algoritmo para o particionamento:
1. Escolha arbitrariamente um pivô x.
2. Percorra o vetor a partir da esquerda até que v[i] >= x.
3. Percorra o vetor a partir da direita até que v[j] <= x.
4. Troque v[i] com v[j]
5. Continue este processo até que os apontadores i e j se cruzarem.
Concluído o particionamento, o vetor v[esq..dir] está particionado de tal forma que:
- Os itens em v[esq], v[esq + 1],..., v[j] são menores ou iguais a x.
- Os itens em v[i], v[i + 1],...,v[dir] são maiores ou iguais a x.

Pode-se concluir então que os valores iguais a x encontram-se na posição correta de ordenação.

### Quick Sort - Exemplo
<img src="https://upload.wikimedia.org/wikipedia/commons/9/9c/Quicksort-example.gif">

```py
def quick_sort(A):
    comps, trocas = quicksort_helper(A, 0, len(A) - 1)
    return trocas, comps

def quicksort_helper(A, esquerda, direita):
    trocas = 0
    comps = 0
    if esquerda < direita:
        indice_pivo, comps_partition = particao(A, esquerda, direita)
        trocas += 1
        comps += comps_partition
        comps_left, trocas_left = quicksort_helper(A, esquerda, indice_pivo - 1)
        comps_right, trocas_right = quicksort_helper(A, indice_pivo + 1, direita)
        comps += comps_left + comps_right
        trocas += trocas_left + trocas_right

    return comps, trocas

def particao(A, esquerda, direita):
    meio = (esquerda + direita) // 2
    if A[meio] < A[esquerda]:
        A[meio], A[esquerda] = A[esquerda], A[meio]
    if A[direita] < A[esquerda]:
        A[direita], A[esquerda] = A[esquerda], A[direita]
    if A[direita] < A[meio]:
        A[direita], A[meio] = A[meio], A[direita]
        
    A[meio], A[direita - 1] = A[direita - 1], A[meio]
    pivo = A[direita - 1]
    i = esquerda
    j = direita - 1
    
    while True:
        i += 1
        while A[i] < pivo:
            i += 1
            
        j -= 1
        while A[j] > pivo:
            j -= 1

        if i < j:
            A[i], A[j] = A[j], A[i]
        else:
            break

    A[i], A[direita - 1] = A[direita - 1], A[i]
    
    return i, j - esquerda + 1
```

- Melhor Caso: `O(n log n)`;
- Ocorre quando o problema é sempre dividido em subproblemas de igual tamanho após a partição.
- Pior Caso: `O(n²)`;
- Ocorre quando sistematicamente, o pivô é escolhido como sendo um dos extremos de um arquivo já ordenado.
- O pior caso pode ser evitado empregando pequenas modificações no algoritmo:
    - Escolher o pivô aleatoriamente.
    - Escolher três itens quaisquer do vetor e usar a mediana dos três como pivô
- Caso médio: 1.386n log n - 0.646n = `O(n log n)`
- Análise por Sedgewick e Flajolet (1996)
- A proporção das divisões não será sempre constante.
- Ocorre quando há uma mistura de divisões boas e ruins.
- O caso médio está muito mais próximo do melhor do que do pior caso.

## Algorimos Gulosos (Greedy Algorithm)
### Justificativa
Algoritmos para problemas de otimização tipicamente realizam uma sequência de passos, como um conjunto de opções a cada passo. Para muito destes problemas, utilizar busca completa ou outros paradigmas pode ser lento demais, algoritmos mais "simples" ou mais "espertos", os resolveriam. Ainda, o problema pode ser tão difícil em uma troca de uma solução "razoavelmente boa".
### Método Guloso
- `Ideia:` Quando temos uma escolha a fazer, fazemos aquela que pareça ser a melhor no momento.
    - Ou sejam uma escolha `ótima local`, na esperança de um solução `ótima global`.
- O método guloso sugere construir uma solução através de uma `sequência de passos`, cda um expandindo uma solução parcialmente construída até o momento, até uma solução completa para o problema ser obtida.
    - Métodos gulosos nem sempre garantem soluções ótimas, mas quando eles garantem, geralmente são as soluções mais simples e eficientes.
- Em cada passo de um algoritmo guloso, a escolha deve ser:
    - `Possível:` deve satisfazer as restrições do problema;
    - `Localmente ótima:` deve ser a melhor escolha local dentre todas as escolhas disponíveis naquele passo;
    - `Irreversível:` uma vez feita, ele não pode ser alterada nos passos subsequentes do algoritmo.
### Características
- O processo de construção da solução é iterativo, dividido em passos. A cada passo, existe um conjunto ou lista de elementos candidatos a fazerem parte da solução, a cada passo, um destes elementos candidatos é adicionado à solução. Deve haver uma maneira de verificar se um conjunto específico de elementos produzem uma solução completa ou uma solução parcial, deve haver também uma maneira de verificar se uma dada solução parcial é viável ou não. Uma função de seleção determina a cada passo, entre os elementos candidatos, qual é o mais promissor. A função de avaliação determina o valor da solução encontrada, seja ela completa ou parcial.
### Caracterização dos Problemas
- Como determinar se um problema pode ser resolvido via algoritmo guloso? Sabemos que algoritmos gulosos nem sempre funcionam
- Um problema deve possuir duas características para que os algoritmos gulosos sejam suficientes:
    - `Subestrutura Ótima:` A solução do problema original é formada pelas soluções ótimas dos subproblemas;
    - `Propriedade Gulosa:` Se fizermos uma escolha que parece a melhor no momento e resolvermos os demais subproblemas da mesma maneira, ainda assim podemos atingir a solução ótima. `Nunca` é necessário reconsiderar as decições tomadas anteriormente.

### Ótimo x Subótimo
Algoritmos gulosos podem ser aplicados a problemas que não possuam subestrutura ótima ou propriedade gulosa, ao custo de não haver garantia de qualidade da solução. Com efeito, para muitos problemas não se conhecem algoritmos exatos(os quais sempre determinam as soluções ótimas globais) eficientes. Nestas situações, torna-se interessante trocarmos a resposta ótima(aparantemente inalcançável) por uma resposta subótima (e alcançável com certa "facilidade"). Mesmo não havendo qualquer garantia sobre a qualidade da solução, um bom algoritmo heurístico guloso ainda pode encontrar as soluções ótimas para instâncias de um problema.
### Vantagens
- Algoritmo Simples;
- Fácil implementação;
- Quando garantem soluções ótimas, geralmente são os algoritmos mais simples e eficientes.
### Desvantagens
- Nem sempre garantem soluções ótimas locais;
- Podem efetuar cálculos repetitivos;
- Escolhe o caminho que, à primeira vista, é mais econômico.

# Exemplo - Coin Change
- `Problema:` Dado um conjunto de moedas com diferentes valores e uma quantia total, encontre o número mínimo de moedas necessárias para fazer o troco.
- `Abordagem Gulosa:` Sempre escolha a maior moeda possível que seja menor ou igual à quantia restante.
- `Análise:` Esta estratégia funciona porque, ao escolher a maior moeda disponível em cada etapa, você está minimizando o número total de moedas necessárias.

# Exemplo - Máximo Global - Exemplo de falha
- `Problema:` Dado um conjunto de subproblemas, maximize cada subproblema para obter uma solução global otimizada.
- `Exemplo de Falha:` Nem todos os problemas têm soluções ótimas locais que levam a uma solução global ótima. Às vezes, a escolha local pode levar a uma solução subótima.
- `Análise:` Nem todos os problemas são adequados para abordagens gulosas. A falha neste exemplo destaca a importância de escolher a abordagem certa para um problema específico.

# Exemplo - Caminho mais Custoso - Exemplo de falha
- `Problema:` Encontrar o caminho mais custoso em um grafo direcionado ponderado, onde cada aresta tem um peso associado.
- `Exemplo de Falha:` Escolher o caminho mais custoso localmente pode não levar à solução global mais custosa.
- `Análise:` O fato de escolher a opção mais custosa localmente não garante a otimalidade global. Neste caso, é necessária uma abordagem mais complexa, como programação dinâmica.

# Exemplo - Problema da Mochila
- `Problema:` Dado um conjunto de itens com pesos e valores, determine a quantidade máxima de valor que pode ser colocado em uma mochila de capacidade limitada.
- `Abordagem Gulosa:` Ordene os itens por valor/peso e, em seguida, coloque o máximo possível dos itens mais valiosos, respeitando a capacidade da mochila.
- `Análise:` Esta abordagem funciona porque, ao escolher itens com a melhor relação valor/peso primeiro, você está maximizando o valor total da mochila.

## Programação Orientada a Objetos
A Programação Orientada a Objetos (POO) é um paradigma de programação baseado no conceito de "objetos", que podem conter dados na forma de atributos e comportamentos na forma de métodos. Esse paradigma foi inicialmente desenvolvido na linguagem Simula 67 e evoluiu significativamente com a chegada da linguagem Smalltalk 80, que é frequentemente considerada um modelo base para linguagens puramente orientadas a objetos.

### Principais Conceitos da POO:
1. `Tipos de Dados Abstratos:`
- Na POO, os tipos de dados abstratos são representados por "objetos". Cada objeto encapsula dados (atributos) e comportamentos (métodos) relacionados a uma entidade do mundo real.
2. `Herança:`
- Herança é um mecanismo que permite que uma classe herde os atributos e métodos de outra classe, chamada de superclasse ou classe mãe. Isso promove a reutilização de código e a organização hierárquica de classes.
3. `Vinculação Dinâmica de Chamadas a Métodos:`
- A vinculação dinâmica (ou polimorfismo) permite que métodos com a mesma assinatura se comportem de maneiras diferentes dependendo do tipo real do objeto durante a execução.
### Linguagens Orientadas a Objetos:
1. `Smalltalk:`
- Considerada uma linguagem puramente orientada a objetos, Smalltalk foi uma das primeiras a implementar totalmente os conceitos de POO.
2. `Java e C#:`
- Java e C# são exemplos de linguagens modernas de programação orientada a objetos que não implementam mais conceitos de linguagens procedurais. Elas são amplamente utilizadas e consideradas exemplos de sucesso da POO.
3. `Python:`
- Python também é uma linguagem orientada a objetos, mas mantém algumas características da programação estruturada. É conhecida por sua simplicidade e flexibilidade.
### Encapsulamento e Acesso a Membros:
- `Encapsulamento:`
    - Em POO, o encapsulamento é alcançado por meio da implementação de classes. Uma classe é uma declaração de um tipo de objeto que encapsula seus dados (atributos) e comportamentos (métodos).
- `Atributos e Métodos:`
    - Atributos representam as propriedades de uma classe, enquanto os métodos são as operações ou funções associadas à classe.
- `Modificadores de Acesso:`
    - A visibilidade de atributos e métodos é controlada por modificadores de acesso como public, private, e protected. O encapsulamento protege os dados de uso indevido.
### Herança e Polimorfismo:
- `Herança:`
- A herança permite que uma classe herde os atributos e métodos de outra. A classe que herda é chamada de subclasse, enquanto a classe da qual é herdado é a superclasse.
- `Polimorfismo:`
- O polimorfismo permite que objetos de diferentes classes sejam tratados de maneira uniforme. Isso é alcançado por meio de métodos com a mesma assinatura em classes diferentes.
### Interfaces e Classes Abstratas:
- `Interfaces:`
    - Interfaces definem contratos que as classes devem seguir, especificando métodos sem implementação. Uma classe pode implementar várias interfaces.
- `Classes Abstratas:`
    - Classes abstratas são modelos para classes concretas e podem conter métodos abstratos, que devem ser implementados por classes derivadas.

### Exemplo Prático em Java:
```java
// Exemplo de uma classe em Java
public class Pessoa {
    private String nome;
    private String cpf;

    public Pessoa(String nome, String cpf) {
        this.nome = nome;
        this.cpf = cpf;
    }

    public void tirarCopias() {
        System.out.println("Tirando cópias...");
    }
}

// Exemplo de uma subclasse (Aluno) que herda de Pessoa
public class Aluno extends Pessoa {
    private String matricula;

    public Aluno(String nome, String cpf, String matricula) {
        super(nome, cpf); // Chama o construtor da superclasse
        this.matricula = matricula;
    }

    @Override
    public void tirarCopias() {
        System.out.println("Aluno tirando cópias com desconto...");
    }
}
```

Este exemplo ilustra alguns dos conceitos discutidos, incluindo encapsulamento, herança, polimorfismo e classes abstratas (através do método tirarCopias).

### Conclusão:
> A Programação Orientada a Objetos oferece uma abordagem modular e flexível para o desenvolvimento de software, promovendo a reutilização de código, organização hierárquica e modelagem mais próxima do mundo real. Seus conceitos fundamentais, como encapsulamento, herança, polimorfismo, interfaces e classes abstratas, fornecem ferramentas poderosas para criar sistemas robustos e extensíveis.